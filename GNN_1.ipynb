{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP8RgwM0H97nbj+kZoDGcO6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nhut-ngnn/Practice-Graph_Neural_Network/blob/main/GNN_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2S0khpCqo65e"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "os.environ['PYTHONWARNINGS'] = \"ignore\"\n",
        "# !pip install torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "# !pip install torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "# !pip install git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "metadata": {
        "id": "Nx4abmrEp2FB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "source": [
        "!pip install torch-geometric  # Install PyTorch Geometric directly\n",
        "from torch_geometric.datasets import Planetoid"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "dbGzT5hMvfKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "\n",
        "dataset = Planetoid(root='data/Planetoid', name='Cora', transform=NormalizeFeatures())\n",
        "\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('======================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "\n",
        "data = dataset[0]  # Get the first graph object.\n",
        "print(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnyhs8Iup8T_",
        "outputId": "a1a6595f-5f27-4b4f-d662-6feabb01c880"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: Cora():\n",
            "======================\n",
            "Number of graphs: 1\n",
            "Number of features: 1433\n",
            "Number of classes: 7\n",
            "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GCNConv\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(1234567)\n",
        "        self.conv1 = GCNConv(dataset.num_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "model = GCN(hidden_channels=16)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVWL4dGuvR_-",
        "outputId": "161a47bc-e57c-466c-d0eb-92801dc9a1ac"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN(\n",
            "  (conv1): GCNConv(1433, 16)\n",
            "  (conv2): GCNConv(16, 7)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GCN(hidden_channels=16)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train():\n",
        "      model.train()\n",
        "      optimizer.zero_grad()\n",
        "      out = model(data.x, data.edge_index)\n",
        "      loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      return loss\n",
        "\n",
        "def test(mask):\n",
        "      model.eval()\n",
        "      out = model(data.x, data.edge_index)\n",
        "      pred = out.argmax(dim=1)\n",
        "      correct = pred[mask] == data.y[mask]\n",
        "      acc = int(correct.sum()) / int(mask.sum())\n",
        "      return acc\n",
        "\n",
        "\n",
        "val_acc_all = []\n",
        "test_acc_all = []\n",
        "for epoch in range(1, 101):\n",
        "    loss = train()\n",
        "    val_acc = test(data.val_mask)\n",
        "    test_acc = test(data.test_mask)\n",
        "    val_acc_all.append(val_acc)\n",
        "    test_acc_all.append(test_acc)\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFzcbp9vvyeI",
        "outputId": "10124cb9-a5c7-4c37-a91a-4f47d140cc70"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 1.9463, Val: 0.2880, Test: 0.2700\n",
            "Epoch: 002, Loss: 1.9409, Val: 0.2580, Test: 0.2910\n",
            "Epoch: 003, Loss: 1.9343, Val: 0.2560, Test: 0.2910\n",
            "Epoch: 004, Loss: 1.9275, Val: 0.2640, Test: 0.3210\n",
            "Epoch: 005, Loss: 1.9181, Val: 0.3220, Test: 0.3630\n",
            "Epoch: 006, Loss: 1.9086, Val: 0.4000, Test: 0.4120\n",
            "Epoch: 007, Loss: 1.9015, Val: 0.3900, Test: 0.4010\n",
            "Epoch: 008, Loss: 1.8933, Val: 0.3900, Test: 0.4020\n",
            "Epoch: 009, Loss: 1.8808, Val: 0.4220, Test: 0.4180\n",
            "Epoch: 010, Loss: 1.8685, Val: 0.4560, Test: 0.4470\n",
            "Epoch: 011, Loss: 1.8598, Val: 0.4760, Test: 0.4680\n",
            "Epoch: 012, Loss: 1.8482, Val: 0.5120, Test: 0.5180\n",
            "Epoch: 013, Loss: 1.8290, Val: 0.5380, Test: 0.5440\n",
            "Epoch: 014, Loss: 1.8233, Val: 0.5580, Test: 0.5720\n",
            "Epoch: 015, Loss: 1.8057, Val: 0.5820, Test: 0.5910\n",
            "Epoch: 016, Loss: 1.7966, Val: 0.6060, Test: 0.6080\n",
            "Epoch: 017, Loss: 1.7825, Val: 0.6200, Test: 0.6300\n",
            "Epoch: 018, Loss: 1.7617, Val: 0.6280, Test: 0.6450\n",
            "Epoch: 019, Loss: 1.7491, Val: 0.6280, Test: 0.6520\n",
            "Epoch: 020, Loss: 1.7310, Val: 0.6320, Test: 0.6560\n",
            "Epoch: 021, Loss: 1.7147, Val: 0.6360, Test: 0.6570\n",
            "Epoch: 022, Loss: 1.7056, Val: 0.6420, Test: 0.6640\n",
            "Epoch: 023, Loss: 1.6954, Val: 0.6720, Test: 0.6770\n",
            "Epoch: 024, Loss: 1.6697, Val: 0.6920, Test: 0.6950\n",
            "Epoch: 025, Loss: 1.6538, Val: 0.7080, Test: 0.7140\n",
            "Epoch: 026, Loss: 1.6312, Val: 0.7160, Test: 0.7150\n",
            "Epoch: 027, Loss: 1.6161, Val: 0.7160, Test: 0.7170\n",
            "Epoch: 028, Loss: 1.5899, Val: 0.7200, Test: 0.7230\n",
            "Epoch: 029, Loss: 1.5711, Val: 0.7160, Test: 0.7220\n",
            "Epoch: 030, Loss: 1.5576, Val: 0.7220, Test: 0.7210\n",
            "Epoch: 031, Loss: 1.5393, Val: 0.7260, Test: 0.7280\n",
            "Epoch: 032, Loss: 1.5137, Val: 0.7340, Test: 0.7370\n",
            "Epoch: 033, Loss: 1.4948, Val: 0.7340, Test: 0.7380\n",
            "Epoch: 034, Loss: 1.4913, Val: 0.7360, Test: 0.7430\n",
            "Epoch: 035, Loss: 1.4698, Val: 0.7400, Test: 0.7510\n",
            "Epoch: 036, Loss: 1.3998, Val: 0.7520, Test: 0.7570\n",
            "Epoch: 037, Loss: 1.4041, Val: 0.7540, Test: 0.7600\n",
            "Epoch: 038, Loss: 1.3761, Val: 0.7560, Test: 0.7640\n",
            "Epoch: 039, Loss: 1.3631, Val: 0.7620, Test: 0.7700\n",
            "Epoch: 040, Loss: 1.3258, Val: 0.7620, Test: 0.7800\n",
            "Epoch: 041, Loss: 1.3030, Val: 0.7640, Test: 0.7810\n",
            "Epoch: 042, Loss: 1.3119, Val: 0.7600, Test: 0.7760\n",
            "Epoch: 043, Loss: 1.2519, Val: 0.7580, Test: 0.7760\n",
            "Epoch: 044, Loss: 1.2530, Val: 0.7600, Test: 0.7790\n",
            "Epoch: 045, Loss: 1.2492, Val: 0.7660, Test: 0.7800\n",
            "Epoch: 046, Loss: 1.2205, Val: 0.7640, Test: 0.7790\n",
            "Epoch: 047, Loss: 1.2037, Val: 0.7620, Test: 0.7850\n",
            "Epoch: 048, Loss: 1.1571, Val: 0.7660, Test: 0.7900\n",
            "Epoch: 049, Loss: 1.1700, Val: 0.7620, Test: 0.7920\n",
            "Epoch: 050, Loss: 1.1296, Val: 0.7600, Test: 0.7940\n",
            "Epoch: 051, Loss: 1.0860, Val: 0.7600, Test: 0.7930\n",
            "Epoch: 052, Loss: 1.1080, Val: 0.7600, Test: 0.7910\n",
            "Epoch: 053, Loss: 1.0564, Val: 0.7580, Test: 0.7930\n",
            "Epoch: 054, Loss: 1.0157, Val: 0.7560, Test: 0.7930\n",
            "Epoch: 055, Loss: 1.0362, Val: 0.7580, Test: 0.7920\n",
            "Epoch: 056, Loss: 1.0328, Val: 0.7660, Test: 0.7980\n",
            "Epoch: 057, Loss: 1.0058, Val: 0.7680, Test: 0.8000\n",
            "Epoch: 058, Loss: 0.9865, Val: 0.7680, Test: 0.7970\n",
            "Epoch: 059, Loss: 0.9667, Val: 0.7700, Test: 0.8010\n",
            "Epoch: 060, Loss: 0.9741, Val: 0.7680, Test: 0.8000\n",
            "Epoch: 061, Loss: 0.9769, Val: 0.7700, Test: 0.8030\n",
            "Epoch: 062, Loss: 0.9122, Val: 0.7720, Test: 0.8040\n",
            "Epoch: 063, Loss: 0.8993, Val: 0.7760, Test: 0.8050\n",
            "Epoch: 064, Loss: 0.8769, Val: 0.7760, Test: 0.8050\n",
            "Epoch: 065, Loss: 0.8575, Val: 0.7800, Test: 0.8060\n",
            "Epoch: 066, Loss: 0.8897, Val: 0.7760, Test: 0.8030\n",
            "Epoch: 067, Loss: 0.8312, Val: 0.7720, Test: 0.8060\n",
            "Epoch: 068, Loss: 0.8262, Val: 0.7680, Test: 0.8030\n",
            "Epoch: 069, Loss: 0.8511, Val: 0.7660, Test: 0.8070\n",
            "Epoch: 070, Loss: 0.7711, Val: 0.7700, Test: 0.8070\n",
            "Epoch: 071, Loss: 0.8012, Val: 0.7680, Test: 0.8080\n",
            "Epoch: 072, Loss: 0.7529, Val: 0.7740, Test: 0.8080\n",
            "Epoch: 073, Loss: 0.7525, Val: 0.7740, Test: 0.8070\n",
            "Epoch: 074, Loss: 0.7689, Val: 0.7740, Test: 0.8110\n",
            "Epoch: 075, Loss: 0.7553, Val: 0.7760, Test: 0.8140\n",
            "Epoch: 076, Loss: 0.7032, Val: 0.7780, Test: 0.8120\n",
            "Epoch: 077, Loss: 0.7326, Val: 0.7800, Test: 0.8110\n",
            "Epoch: 078, Loss: 0.7122, Val: 0.7840, Test: 0.8120\n",
            "Epoch: 079, Loss: 0.7090, Val: 0.7880, Test: 0.8110\n",
            "Epoch: 080, Loss: 0.6755, Val: 0.7800, Test: 0.8130\n",
            "Epoch: 081, Loss: 0.6666, Val: 0.7780, Test: 0.8070\n",
            "Epoch: 082, Loss: 0.6679, Val: 0.7700, Test: 0.8080\n",
            "Epoch: 083, Loss: 0.7037, Val: 0.7700, Test: 0.8100\n",
            "Epoch: 084, Loss: 0.6752, Val: 0.7700, Test: 0.8070\n",
            "Epoch: 085, Loss: 0.6266, Val: 0.7680, Test: 0.8100\n",
            "Epoch: 086, Loss: 0.6564, Val: 0.7660, Test: 0.8080\n",
            "Epoch: 087, Loss: 0.6266, Val: 0.7660, Test: 0.8090\n",
            "Epoch: 088, Loss: 0.6411, Val: 0.7660, Test: 0.8080\n",
            "Epoch: 089, Loss: 0.6226, Val: 0.7700, Test: 0.8100\n",
            "Epoch: 090, Loss: 0.6535, Val: 0.7780, Test: 0.8130\n",
            "Epoch: 091, Loss: 0.6317, Val: 0.7820, Test: 0.8140\n",
            "Epoch: 092, Loss: 0.5741, Val: 0.7840, Test: 0.8120\n",
            "Epoch: 093, Loss: 0.5572, Val: 0.7860, Test: 0.8140\n",
            "Epoch: 094, Loss: 0.5710, Val: 0.7820, Test: 0.8120\n",
            "Epoch: 095, Loss: 0.5816, Val: 0.7820, Test: 0.8140\n",
            "Epoch: 096, Loss: 0.5745, Val: 0.7780, Test: 0.8140\n",
            "Epoch: 097, Loss: 0.5547, Val: 0.7740, Test: 0.8150\n",
            "Epoch: 098, Loss: 0.5989, Val: 0.7800, Test: 0.8160\n",
            "Epoch: 099, Loss: 0.6021, Val: 0.7720, Test: 0.8160\n",
            "Epoch: 100, Loss: 0.5799, Val: 0.7780, Test: 0.8150\n"
          ]
        }
      ]
    }
  ]
}